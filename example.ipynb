{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Use Case: RAG Q/A System with Langchain\n",
    "\n",
    "Based on [this tutorial](https://python.langchain.com/docs/tutorials/pdf_qa/) from Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./nike-10k-2023.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What was Nike's revenue in 2023?\"})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing with Weavel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weavel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from weavel import Weavel\n",
    "\n",
    "os.environ[\"WEAVEL_TESTMODE\"] = \"true\"\n",
    "wv = Weavel(\n",
    "    base_url=\"http://localhost:8000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Prompt to Optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common import Prompt\n",
    "\n",
    "base_prompt = Prompt(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{input}\",\n",
    "            }\n",
    "        ],\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Generator\n",
    "\n",
    "A Generator can be implemented by subclassing `BaseGenerator` and implementing the `generate` method.  \n",
    "Given a `Prompt` and inputs, the `generate` method should use your LLM logic (while using the given `Prompt` and `inputs`) and return the outputs. (string / dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from ape.common import Prompt, BaseGenerator\n",
    "\n",
    "class AgentResponse(TypedDict):\n",
    "    context: List[str]\n",
    "    answer: str\n",
    "\n",
    "class LangchainAgentGenerator(BaseGenerator):\n",
    "    def generate(self,\n",
    "        prompt: Prompt,\n",
    "        inputs: Dict[str, Any] = {}\n",
    "    ) -> AgentResponse:\n",
    "        lc_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (m[\"role\"], m[\"content\"]) for m in prompt.format().messages\n",
    "            ]\n",
    "        )\n",
    "        question_answer_chain = create_stuff_documents_chain(llm, lc_prompt)\n",
    "        rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "        results = rag_chain.invoke({\"input\": inputs[\"input\"]})\n",
    "\n",
    "        return AgentResponse(\n",
    "            context=[doc.page_content for doc in results[\"context\"]],\n",
    "            answer=results[\"answer\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator is a `functor` (a callable class), so you can instantiate it and call it like a function.\n",
    "This is used internally by Weavel in the process of prompt optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LangchainAgentGenerator()\n",
    "\n",
    "res = await generator(\n",
    "    prompt=base_prompt,\n",
    "    inputs={\"input\": \"What was Nike's revenue in 2023?\"},\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "In this example, we'll evaluate the RAG system with precision and recall metrics, which don't require a ground truth label.\n",
    "\n",
    "We'll create a trainset and testset of 20 queries each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import textwrap\n",
    "import random\n",
    "\n",
    "dataset_generator = Prompt(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": textwrap.dedent(\n",
    "            \"\"\"\n",
    "            You are an AI assistant tasked with generating user queries.\n",
    "            Given the following chunk of a document, generate a relevant\n",
    "            user query that could be answered using the information in\n",
    "            this chunk. The query should be specific and directly related\n",
    "            to the content provided.\n",
    "\n",
    "            Generate a single, concise user query based on this information.\n",
    "            \"\"\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(\n",
    "            \"\"\"\n",
    "            Please generate a user query based on the given document chunk.\n",
    "\n",
    "            Document chunk:\n",
    "            {context}\n",
    "            \"\"\"\n",
    "            )\n",
    "        },\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "generated_queries = []\n",
    "\n",
    "async def _task(context):\n",
    "    res = await dataset_generator(context=context)\n",
    "    generated_queries.append(res)\n",
    "\n",
    "tasks = [\n",
    "    _task(doc.page_content)\n",
    "    for doc in random.sample(splits, 40)\n",
    "]\n",
    "\n",
    "await asyncio.gather(*tasks)\n",
    "\n",
    "print(generated_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common import DatasetItem\n",
    "\n",
    "trainset = [DatasetItem(inputs={\"input\": query}) for query in generated_queries[:20]]\n",
    "testset = [DatasetItem(inputs={\"input\": query}) for query in generated_queries[20:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "We'll use precision and recall metrics in this tutorial.  \n",
    "A Metric can be implemented by subclassing `BaseMetric` and implementing the `evaluate` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define LLM judge prompts for precision and recall in a `.prompt` file.  \n",
    "`ape.common.Prompt` objects can load and dump from `.prompt` files.  \n",
    "Syntax highlighting for `.prompt` files is supported with the [Promptfile Intellisense](https://marketplace.visualstudio.com/items?itemName=Weavel.promptfile-intellisense) extension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common import BaseMetric, MetricResult\n",
    "from pysbd import Segmenter\n",
    "from pysbd.utils import TextSpan\n",
    "\n",
    "class RAGMetric(BaseMetric):\n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter(language=\"en\", clean=False, char_span=True)\n",
    "\n",
    "        self.analyze_statements = Prompt.load_file(\"./judge-prompts/statement-analysis.prompt\")\n",
    "        self.score_precision = Prompt.load_file(\"./judge-prompts/precision-judge.prompt\")\n",
    "        self.score_recall = Prompt.load_file(\"./judge-prompts/recall-judge.prompt\")\n",
    "\n",
    "    async def compute(self,\n",
    "        dataset_item: DatasetItem,\n",
    "        pred: AgentResponse,\n",
    "    ) -> MetricResult:\n",
    "        sentences: List[TextSpan] = self.segmenter.segment(pred[\"answer\"])\n",
    "        sentences: List[str] = [sentence.sent for sentence in sentences]\n",
    "        sentences = [\n",
    "            sentence.strip()\n",
    "            for sentence in sentences\n",
    "            if sentence.strip().endswith(\".\")\n",
    "        ]\n",
    "        sentences = \"\\n\".join([f\"{i}:{x}\" for i, x in enumerate(sentences)])\n",
    "\n",
    "        res = await self.analyze_statements(\n",
    "            question=dataset_item[\"inputs\"][\"input\"],\n",
    "            answer=pred[\"answer\"],\n",
    "            sentences=sentences,\n",
    "        )\n",
    "        statements = [\n",
    "            statement\n",
    "            for analysis in res[\"analysis\"]\n",
    "            for statement in analysis[\"simpler_statements\"]\n",
    "        ]\n",
    "\n",
    "        ground_truth = \"\\n\\n\".join(pred[\"context\"])\n",
    "\n",
    "        precision_task = self.score_precision(\n",
    "            ground_truth=ground_truth,\n",
    "            statements=statements,\n",
    "        )\n",
    "        recall_task = self.score_recall(\n",
    "            ground_truth=ground_truth,\n",
    "            statements=statements,\n",
    "        )\n",
    "\n",
    "        precision_res, recall_res = await asyncio.gather(precision_task, recall_task)\n",
    "\n",
    "        precision_score = sum(answer['verdict'] for answer in precision_res['answer']) / len(precision_res['answer']) if precision_res['answer'] else 0\n",
    "        recall_score = sum(answer['verdict'] for answer in recall_res['answer']) / len(recall_res['answer']) if recall_res['answer'] else 0\n",
    "\n",
    "        return MetricResult(\n",
    "            score = (precision_score + recall_score) / 2,\n",
    "            trace={\n",
    "                \"precision\": precision_score,\n",
    "                \"recall\": recall_score,\n",
    "            }\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = RAGMetric()\n",
    "\n",
    "await metric(\n",
    "    dataset_item=trainset[0],\n",
    "    pred=await generator(\n",
    "        prompt=base_prompt,\n",
    "        inputs=trainset[0][\"inputs\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize\n",
    "\n",
    "`wv.optimize` will return an optimized prompt based on provided parameters. If you pass in multiple models, it will return the highest performing prompt + model, and one optimized prompt will be saved per each model.\n",
    "\n",
    "An optimization task can take a while to complete, ranging from 5 minutes to around an hour depending on the prompt, metric, and parameters.\n",
    "\n",
    "You can view the optimized prompts on the [Weavel dashboard](https://app.weavel.ai/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from weavel.utils import logger\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently support the following algorithms: **dspy_mipro**, **few_shot**, **text_gradient**, **optuna**, **expel**\n",
    "\n",
    "We are currently in the process of benchmarking these algorithms to determine the best for each use case. It will all be open sourced in [ape-core](https://github.com/weavel-ai/Ape).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt = await wv.optimize(\n",
    "    base_prompt=base_prompt,\n",
    "    models=[\"gpt-4o\"],\n",
    "    trainset=trainset,\n",
    "    metric=metric,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "with open(\"./optimized-prompt.prompt\", \"w\") as f:\n",
    "    f.write(optimized_prompt.dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Use `Evaluator` to evaluate a prompt on a testset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ape.common import Evaluator\n",
    "\n",
    "evaluate = Evaluator(\n",
    "    testset=testset,\n",
    "    metric=metric,\n",
    "    generator=generator,\n",
    "    display_table=True,\n",
    "    display_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate(base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate(optimized_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
